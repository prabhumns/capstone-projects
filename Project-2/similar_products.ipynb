{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-F3ddMzpf4qU"
   },
   "outputs": [],
   "source": [
    "CURR_DIR = \"/tf/Capstone Project/Project-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nWdib-_Jf4qQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/Capstone Project/Project-2\n",
      "/tf/Capstone Project/Project-2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import unicodedata\n",
    "import contractions\n",
    "import os\n",
    "from datetime import datetime\n",
    "from num2words import num2words\n",
    "print(os.getcwd())\n",
    "os.chdir(CURR_DIR)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_df(df):\n",
    "    print(f\"Number of observations: {df.shape[0]}\")\n",
    "    print(f\"Number of variables: {df.shape[1]}\")\n",
    "    print(f\"Number of duplicates: {df.duplicated().sum()}\")\n",
    "    print(f\"Are there any missing values {df.isnull().values.any()}\")\n",
    "    print(\"-----\")\n",
    "    print(df.dtypes.sort_values(ascending=True))\n",
    "    print(\"------\")\n",
    "    print(\"Datatypes' proportion:\")\n",
    "    print(df.dtypes.value_counts(ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_val(df):\n",
    "    detect_null_val = df.isnull().values.any()\n",
    "    if detect_null_val:\n",
    "        null_abs = df.isnull().sum()\n",
    "        null_pc = df.isnull().sum() / df.isnull().shape[0] *100\n",
    "        null_concat = pd.concat([null_abs,null_pc], axis=1).round(2)\n",
    "        null_concat.columns = ['Absolute', 'Percent']\n",
    "        return null_concat.sort_values(by=\"Absolute\", ascending=False)\n",
    "    else:\n",
    "        print(\"There are no missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrs(x):\n",
    "    mask = np.triu(x.corr(), 1)\n",
    "    plt.figure(figsize=(19, 9))\n",
    "    return sns.heatmap(x.corr(), annot=True, vmax=1, vmin=-1, square=True, cmap='BrBG', mask=mask);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_counts(df, thresh = 15):\n",
    "    for column in df.columns:\n",
    "        if df[column].nunique() < thresh:\n",
    "            print(df.groupby([column], dropna = False).size(), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Dz1_0tdnf4qU"
   },
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('./Data/prods.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Duzoj6rUf4qV",
    "outputId": "f8bbf348-3e18-48ab-d691-331a785a42b6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>aisle_id</th>\n",
       "      <th>department_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Chocolate Sandwich Cookies</td>\n",
       "      <td>61</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>All-Seasons Salt</td>\n",
       "      <td>104</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Robust Golden Unsweetened Oolong Tea</td>\n",
       "      <td>94</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Smart Ones Classic Favorites Mini Rigatoni Wit...</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Green Chile Anytime Sauce</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id                                       product_name  aisle_id  \\\n",
       "0           1                         Chocolate Sandwich Cookies        61   \n",
       "1           2                                   All-Seasons Salt       104   \n",
       "2           3               Robust Golden Unsweetened Oolong Tea        94   \n",
       "3           4  Smart Ones Classic Favorites Mini Rigatoni Wit...        38   \n",
       "4           5                          Green Chile Anytime Sauce         5   \n",
       "\n",
       "   department_id  \n",
       "0             19  \n",
       "1             13  \n",
       "2              7  \n",
       "3              1  \n",
       "4             13  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>aisle_id</th>\n",
       "      <th>department_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>500.500000</td>\n",
       "      <td>68.020000</td>\n",
       "      <td>11.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>288.819436</td>\n",
       "      <td>38.800684</td>\n",
       "      <td>5.887058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>250.750000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>500.500000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>750.250000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        product_id     aisle_id  department_id\n",
       "count  1000.000000  1000.000000    1000.000000\n",
       "mean    500.500000    68.020000      11.698000\n",
       "std     288.819436    38.800684       5.887058\n",
       "min       1.000000     1.000000       1.000000\n",
       "25%     250.750000    37.000000       7.000000\n",
       "50%     500.500000    69.000000      12.000000\n",
       "75%     750.250000   101.000000      17.000000\n",
       "max    1000.000000   134.000000      21.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw['department_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw['aisle_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations: 1000\n",
      "Number of variables: 4\n",
      "Number of duplicates: 0\n",
      "Are there any missing values False\n",
      "-----\n",
      "product_id        int64\n",
      "aisle_id          int64\n",
      "department_id     int64\n",
      "product_name     object\n",
      "dtype: object\n",
      "------\n",
      "Datatypes' proportion:\n",
      "object    1\n",
      "int64     3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "shape_df(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no missing values.\n"
     ]
    }
   ],
   "source": [
    "null_val(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aisle_id\n",
      "1       3\n",
      "2       5\n",
      "3      21\n",
      "4      12\n",
      "5       8\n",
      "       ..\n",
      "130     8\n",
      "131    13\n",
      "132     2\n",
      "133     2\n",
      "134     2\n",
      "Length: 128, dtype: int64 \n",
      "\n",
      "\n",
      "department_id\n",
      "1      87\n",
      "2       5\n",
      "3      31\n",
      "4      33\n",
      "5      23\n",
      "6      19\n",
      "7      91\n",
      "8      23\n",
      "9      40\n",
      "10      2\n",
      "11    141\n",
      "12     16\n",
      "13     86\n",
      "14     28\n",
      "15     39\n",
      "16     58\n",
      "17     76\n",
      "18     20\n",
      "19    130\n",
      "20     34\n",
      "21     18\n",
      "dtype: int64 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_counts(df_raw, thresh = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy() #Working on a copy of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some text cleaning functions used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing HTML Tags\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    print('Removing HTML Tags, the text can be as big as entire wepage')\n",
    "    return BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "\n",
    "#Removing Accented characters\n",
    "import unicodedata\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    print(\"Removing accented characters, which convert résumé to resumve\")\n",
    "    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_text\n",
    "\n",
    "\n",
    "# \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂\" ==> 'Well this was fun See you at  What do you think  '\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    print(\"Removing special characters like smileys from the text\")\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "#Removing stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "new_stop_words = []\n",
    "all_stop_words = stop_words.union(new_stop_words)\n",
    "\n",
    "not_stopwords = {'no', 'not'}\n",
    "final_stop_words = set(\n",
    "    [word for word in all_stop_words if word not in not_stopwords]\n",
    ")\n",
    "def remove_stop_words(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word not in stop_words:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)\n",
    "            \n",
    "#Removing symbols (Without apostrophe)\n",
    "symbols = \"!\\\"#$%&()*+-./:;<=>?,@[\\]^_`{|}~\\n\"\n",
    "def remove_punctuation(text):\n",
    "    for i in symbols:\n",
    "        text = np.char.replace(text, i, ' ')\n",
    "    return str(text)\n",
    "\n",
    "#Convers popin' to poping\n",
    "def convert_to_ing_words(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word[-3:] == \"in'\":\n",
    "            new_text.append(word[:-3] + \"ing\")\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "#Conver Numeric values\n",
    "def convert_numbers_to_words(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if (word.isnumeric()):\n",
    "            new_text.extend(num2words(word).split())\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)\n",
    "            \n",
    "    \n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def pos_tag_wordnet(tagged_tokens):\n",
    "    tag_map = {'j': wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV}\n",
    "    new_tagged_tokens = [(word, tag_map.get(tag[0].lower(), wordnet.NOUN))\n",
    "                            for word, tag in tagged_tokens]\n",
    "    return new_tagged_tokens\n",
    "\n",
    "def wordnet_lemmatize_text(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tagged_tokens = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    wordnet_tokens = pos_tag_wordnet(tagged_tokens)\n",
    "    lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>aisle_id</th>\n",
       "      <th>department_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>206</td>\n",
       "      <td>Roasted Vegetable Soufflé</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tri-Vi-Sol® Vitamins A-C-and D Supplement Drop...</td>\n",
       "      <td>47</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>150</td>\n",
       "      <td>Brut Rosé</td>\n",
       "      <td>134</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     product_id                                       product_name  aisle_id  \\\n",
       "205         206                          Roasted Vegetable Soufflé        38   \n",
       "23           24  Tri-Vi-Sol® Vitamins A-C-and D Supplement Drop...        47   \n",
       "149         150                                          Brut Rosé       134   \n",
       "\n",
       "     department_id  \n",
       "205              1  \n",
       "23              11  \n",
       "149              5  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Some product names with special and accented characters\n",
    "df.iloc[[205, 23, 149]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ima': 'I am going to',\n",
       " 'gonna': 'going to',\n",
       " 'gotta': 'got to',\n",
       " 'wanna': 'want to',\n",
       " 'woulda': 'would have',\n",
       " 'gimme': 'give me',\n",
       " 'asap': 'as soon as possible',\n",
       " 'u': 'you',\n",
       " 'r ': 'are ',\n",
       " 'Im': 'I am',\n",
       " \"I'm\": 'I am',\n",
       " 'Ima': 'I am about to',\n",
       " \"Im'a\": 'I am about to',\n",
       " \"I'ma\": 'I am about to',\n",
       " \"I'm'a\": 'I am about to',\n",
       " 'Imo': 'I am going to',\n",
       " \"Im'o\": 'I am going to',\n",
       " \"I'mo\": 'I am going to',\n",
       " \"I'm'o\": 'I am going to',\n",
       " 'Ive': 'I have',\n",
       " \"I've\": 'I have',\n",
       " 'Illve': 'I will have',\n",
       " \"Ill've\": 'I will have',\n",
       " \"I'llve\": 'I will have',\n",
       " \"I'll've\": 'I will have',\n",
       " 'Idve': 'I would have',\n",
       " \"Id've\": 'I would have',\n",
       " \"I'dve\": 'I would have',\n",
       " \"I'd've\": 'I would have',\n",
       " 'amnt': 'am not',\n",
       " \"amn't\": 'am not',\n",
       " 'aint': 'are not',\n",
       " \"ain't\": 'are not',\n",
       " 'arent': 'are not',\n",
       " \"aren't\": 'are not',\n",
       " 'cause': 'because',\n",
       " \"'cause\": 'because',\n",
       " 'cant': 'cannot',\n",
       " \"can't\": 'cannot',\n",
       " 'cantve': 'cannot have',\n",
       " \"cant've\": 'cannot have',\n",
       " \"can'tve\": 'cannot have',\n",
       " \"can't've\": 'cannot have',\n",
       " 'couldve': 'could have',\n",
       " \"could've\": 'could have',\n",
       " 'couldnt': 'could not',\n",
       " \"couldn't\": 'could not',\n",
       " 'couldntve': 'could not have',\n",
       " \"couldnt've\": 'could not have',\n",
       " \"couldn'tve\": 'could not have',\n",
       " \"couldn't've\": 'could not have',\n",
       " 'darent': 'dare not',\n",
       " \"daren't\": 'dare not',\n",
       " 'daresnt': 'dare not',\n",
       " \"daresn't\": 'dare not',\n",
       " 'dasnt': 'dare not',\n",
       " \"dasn't\": 'dare not',\n",
       " 'didnt': 'did not',\n",
       " \"didn't\": 'did not',\n",
       " 'dont': 'do not',\n",
       " \"don't\": 'do not',\n",
       " 'doesnt': 'does not',\n",
       " \"doesn't\": 'does not',\n",
       " 'eer': 'ever',\n",
       " \"e'er\": 'ever',\n",
       " 'everyones': 'everyone is',\n",
       " \"everyone's\": 'everyone is',\n",
       " 'gont': 'go not',\n",
       " \"gon't\": 'go not',\n",
       " 'hadnt': 'had not',\n",
       " \"hadn't\": 'had not',\n",
       " 'hadntve': 'had not have',\n",
       " \"hadnt've\": 'had not have',\n",
       " \"hadn'tve\": 'had not have',\n",
       " \"hadn't've\": 'had not have',\n",
       " 'hasnt': 'has not',\n",
       " \"hasn't\": 'has not',\n",
       " 'havent': 'have not',\n",
       " \"haven't\": 'have not',\n",
       " 'heve': 'he have',\n",
       " \"he've\": 'he have',\n",
       " 'hellve': 'he will have',\n",
       " \"hell've\": 'he will have',\n",
       " \"he'llve\": 'he will have',\n",
       " \"he'll've\": 'he will have',\n",
       " 'hed': 'he would',\n",
       " \"he'd\": 'he would',\n",
       " 'hedve': 'he would have',\n",
       " \"hed've\": 'he would have',\n",
       " \"he'dve\": 'he would have',\n",
       " \"he'd've\": 'he would have',\n",
       " 'heres': 'here is',\n",
       " \"here's\": 'here is',\n",
       " 'howre': 'how are',\n",
       " \"how're\": 'how are',\n",
       " 'howd': 'how did',\n",
       " \"how'd\": 'how did',\n",
       " 'howdy': 'how do you',\n",
       " \"howd'y\": 'how do you',\n",
       " \"how'dy\": 'how do you',\n",
       " \"how'd'y\": 'how do you',\n",
       " 'hows': 'how is',\n",
       " \"how's\": 'how is',\n",
       " 'howll': 'how will',\n",
       " \"how'll\": 'how will',\n",
       " 'isnt': 'is not',\n",
       " \"isn't\": 'is not',\n",
       " 'tis': 'it is',\n",
       " \"'tis\": 'it is',\n",
       " 'twas': 'it was',\n",
       " \"'twas\": 'it was',\n",
       " 'itll': 'it will',\n",
       " \"it'll\": 'it will',\n",
       " 'itllve': 'it will have',\n",
       " \"itll've\": 'it will have',\n",
       " \"it'llve\": 'it will have',\n",
       " \"it'll've\": 'it will have',\n",
       " 'itd': 'it would',\n",
       " \"it'd\": 'it would',\n",
       " 'itdve': 'it would have',\n",
       " \"itd've\": 'it would have',\n",
       " \"it'dve\": 'it would have',\n",
       " \"it'd've\": 'it would have',\n",
       " 'lets': 'let us',\n",
       " \"let's\": 'let us',\n",
       " 'maam': 'madam',\n",
       " \"ma'am\": 'madam',\n",
       " 'mayve': 'may have',\n",
       " \"may've\": 'may have',\n",
       " 'maynt': 'may not',\n",
       " \"mayn't\": 'may not',\n",
       " 'mightve': 'might have',\n",
       " \"might've\": 'might have',\n",
       " 'mightnt': 'might not',\n",
       " \"mightn't\": 'might not',\n",
       " 'mightntve': 'might not have',\n",
       " \"mightnt've\": 'might not have',\n",
       " \"mightn'tve\": 'might not have',\n",
       " \"mightn't've\": 'might not have',\n",
       " 'mustve': 'must have',\n",
       " \"must've\": 'must have',\n",
       " 'mustnt': 'must not',\n",
       " \"mustn't\": 'must not',\n",
       " 'mustntve': 'must not have',\n",
       " \"mustnt've\": 'must not have',\n",
       " \"mustn'tve\": 'must not have',\n",
       " \"mustn't've\": 'must not have',\n",
       " 'neednt': 'need not',\n",
       " \"needn't\": 'need not',\n",
       " 'needntve': 'need not have',\n",
       " \"neednt've\": 'need not have',\n",
       " \"needn'tve\": 'need not have',\n",
       " \"needn't've\": 'need not have',\n",
       " 'neer': 'never',\n",
       " \"ne'er\": 'never',\n",
       " 'oclock': 'of the clock',\n",
       " \"o'clock\": 'of the clock',\n",
       " 'ol': 'old',\n",
       " \"ol'\": 'old',\n",
       " 'oughtnt': 'ought not',\n",
       " \"oughtn't\": 'ought not',\n",
       " 'oughtntve': 'ought not have',\n",
       " \"oughtnt've\": 'ought not have',\n",
       " \"oughtn'tve\": 'ought not have',\n",
       " \"oughtn't've\": 'ought not have',\n",
       " 'oer': 'over',\n",
       " \"o'er\": 'over',\n",
       " 'shant': 'shall not',\n",
       " \"shan't\": 'shall not',\n",
       " \"sha'nt\": 'shall not',\n",
       " \"sha'n't\": 'shall not',\n",
       " 'shallnt': 'shall not',\n",
       " \"shalln't\": 'shall not',\n",
       " 'shantve': 'shall not have',\n",
       " \"shant've\": 'shall not have',\n",
       " \"shan'tve\": 'shall not have',\n",
       " \"shan't've\": 'shall not have',\n",
       " 'shes': 'she is',\n",
       " \"she's\": 'she is',\n",
       " 'shell': 'she will',\n",
       " \"she'll\": 'she will',\n",
       " 'shed': 'she would',\n",
       " \"she'd\": 'she would',\n",
       " 'shedve': 'she would have',\n",
       " \"shed've\": 'she would have',\n",
       " \"she'dve\": 'she would have',\n",
       " \"she'd've\": 'she would have',\n",
       " 'shouldve': 'should have',\n",
       " \"should've\": 'should have',\n",
       " 'shouldnt': 'should not',\n",
       " \"shouldn't\": 'should not',\n",
       " 'shouldntve': 'should not have',\n",
       " \"shouldnt've\": 'should not have',\n",
       " \"shouldn'tve\": 'should not have',\n",
       " \"shouldn't've\": 'should not have',\n",
       " 'sove': 'so have',\n",
       " \"so've\": 'so have',\n",
       " 'sos': 'so is',\n",
       " \"so's\": 'so is',\n",
       " 'somebodys': 'somebody is',\n",
       " \"somebody's\": 'somebody is',\n",
       " 'someones': 'someone is',\n",
       " \"someone's\": 'someone is',\n",
       " 'somethings': 'something is',\n",
       " \"something's\": 'something is',\n",
       " 'thatre': 'that are',\n",
       " \"that're\": 'that are',\n",
       " 'thats': 'that is',\n",
       " \"that's\": 'that is',\n",
       " 'thatll': 'that will',\n",
       " \"that'll\": 'that will',\n",
       " 'thatd': 'that would',\n",
       " \"that'd\": 'that would',\n",
       " 'thatdve': 'that would have',\n",
       " \"thatd've\": 'that would have',\n",
       " \"that'dve\": 'that would have',\n",
       " \"that'd've\": 'that would have',\n",
       " 'therere': 'there are',\n",
       " \"there're\": 'there are',\n",
       " 'theres': 'there is',\n",
       " \"there's\": 'there is',\n",
       " 'therell': 'there will',\n",
       " \"there'll\": 'there will',\n",
       " 'thered': 'there would',\n",
       " \"there'd\": 'there would',\n",
       " 'theredve': 'there would have',\n",
       " \"thered've\": 'there would have',\n",
       " \"there'dve\": 'there would have',\n",
       " \"there'd've\": 'there would have',\n",
       " 'thesere': 'these are',\n",
       " \"these're\": 'these are',\n",
       " 'theyre': 'they are',\n",
       " \"they're\": 'they are',\n",
       " 'theyve': 'they have',\n",
       " \"they've\": 'they have',\n",
       " 'theyll': 'they will',\n",
       " \"they'll\": 'they will',\n",
       " 'theyllve': 'they will have',\n",
       " \"theyll've\": 'they will have',\n",
       " \"they'llve\": 'they will have',\n",
       " \"they'll've\": 'they will have',\n",
       " 'theyd': 'they would',\n",
       " \"they'd\": 'they would',\n",
       " 'theydve': 'they would have',\n",
       " \"theyd've\": 'they would have',\n",
       " \"they'dve\": 'they would have',\n",
       " \"they'd've\": 'they would have',\n",
       " 'thiss': 'this is',\n",
       " \"this's\": 'this is',\n",
       " 'thosere': 'those are',\n",
       " \"those're\": 'those are',\n",
       " 'tove': 'to have',\n",
       " \"to've\": 'to have',\n",
       " 'wasnt': 'was not',\n",
       " \"wasn't\": 'was not',\n",
       " 'weve': 'we have',\n",
       " \"we've\": 'we have',\n",
       " 'wellve': 'we will have',\n",
       " \"well've\": 'we will have',\n",
       " \"we'llve\": 'we will have',\n",
       " \"we'll've\": 'we will have',\n",
       " 'wedve': 'we would have',\n",
       " \"wed've\": 'we would have',\n",
       " \"we'dve\": 'we would have',\n",
       " \"we'd've\": 'we would have',\n",
       " 'werent': 'were not',\n",
       " \"weren't\": 'were not',\n",
       " 'whatre': 'what are',\n",
       " \"what're\": 'what are',\n",
       " 'whatd': 'what did',\n",
       " \"what'd\": 'what did',\n",
       " 'whatve': 'what have',\n",
       " \"what've\": 'what have',\n",
       " 'whats': 'what is',\n",
       " \"what's\": 'what is',\n",
       " 'whatll': 'what will',\n",
       " \"what'll\": 'what will',\n",
       " 'whatllve': 'what will have',\n",
       " \"whatll've\": 'what will have',\n",
       " \"what'llve\": 'what will have',\n",
       " \"what'll've\": 'what will have',\n",
       " 'whenve': 'when have',\n",
       " \"when've\": 'when have',\n",
       " 'whens': 'when is',\n",
       " \"when's\": 'when is',\n",
       " 'wherere': 'where are',\n",
       " \"where're\": 'where are',\n",
       " 'whered': 'where did',\n",
       " \"where'd\": 'where did',\n",
       " 'whereve': 'where have',\n",
       " \"where've\": 'where have',\n",
       " 'wheres': 'where is',\n",
       " \"where's\": 'where is',\n",
       " 'whichs': 'which is',\n",
       " \"which's\": 'which is',\n",
       " 'whove': 'who have',\n",
       " \"who've\": 'who have',\n",
       " 'whos': 'who is',\n",
       " \"who's\": 'who is',\n",
       " 'wholl': 'who will',\n",
       " \"who'll\": 'who will',\n",
       " 'whollve': 'who will have',\n",
       " \"wholl've\": 'who will have',\n",
       " \"who'llve\": 'who will have',\n",
       " \"who'll've\": 'who will have',\n",
       " 'whod': 'who would',\n",
       " \"who'd\": 'who would',\n",
       " 'whodve': 'who would have',\n",
       " \"whod've\": 'who would have',\n",
       " \"who'dve\": 'who would have',\n",
       " \"who'd've\": 'who would have',\n",
       " 'whyre': 'why are',\n",
       " \"why're\": 'why are',\n",
       " 'whyd': 'why did',\n",
       " \"why'd\": 'why did',\n",
       " 'whyve': 'why have',\n",
       " \"why've\": 'why have',\n",
       " 'whys': 'why is',\n",
       " \"why's\": 'why is',\n",
       " 'willve': 'will have',\n",
       " \"will've\": 'will have',\n",
       " 'wont': 'will not',\n",
       " \"won't\": 'will not',\n",
       " 'wontve': 'will not have',\n",
       " \"wont've\": 'will not have',\n",
       " \"won'tve\": 'will not have',\n",
       " \"won't've\": 'will not have',\n",
       " 'wouldve': 'would have',\n",
       " \"would've\": 'would have',\n",
       " 'wouldnt': 'would not',\n",
       " \"wouldn't\": 'would not',\n",
       " 'wouldntve': 'would not have',\n",
       " \"wouldnt've\": 'would not have',\n",
       " \"wouldn'tve\": 'would not have',\n",
       " \"wouldn't've\": 'would not have',\n",
       " 'yall': 'you all',\n",
       " \"y'all\": 'you all',\n",
       " 'yallre': 'you all are',\n",
       " \"yall're\": 'you all are',\n",
       " \"y'allre\": 'you all are',\n",
       " \"y'all're\": 'you all are',\n",
       " 'yallve': 'you all have',\n",
       " \"yall've\": 'you all have',\n",
       " \"y'allve\": 'you all have',\n",
       " \"y'all've\": 'you all have',\n",
       " 'yalld': 'you all would',\n",
       " \"yall'd\": 'you all would',\n",
       " \"y'alld\": 'you all would',\n",
       " \"y'all'd\": 'you all would',\n",
       " 'yalldve': 'you all would have',\n",
       " \"yalld've\": 'you all would have',\n",
       " \"yall'dve\": 'you all would have',\n",
       " \"yall'd've\": 'you all would have',\n",
       " \"y'alldve\": 'you all would have',\n",
       " \"y'alld've\": 'you all would have',\n",
       " \"y'all'dve\": 'you all would have',\n",
       " \"y'all'd've\": 'you all would have',\n",
       " 'youre': 'you are',\n",
       " \"you're\": 'you are',\n",
       " 'youve': 'you have',\n",
       " \"you've\": 'you have',\n",
       " 'youllve': 'you shall have',\n",
       " \"youll've\": 'you shall have',\n",
       " \"you'llve\": 'you shall have',\n",
       " \"you'll've\": 'you shall have',\n",
       " 'youll': 'you will',\n",
       " \"you'll\": 'you will',\n",
       " 'youd': 'you would',\n",
       " \"you'd\": 'you would',\n",
       " 'youdve': 'you would have',\n",
       " \"youd've\": 'you would have',\n",
       " \"you'dve\": 'you would have',\n",
       " \"you'd've\": 'you would have'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#existing key-value pairs in contractions library\n",
    "contractions.slang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Few added contraction key-value pairs to fix the dialect of the names\n",
    "contractions.add(\"lil'\", 'little')\n",
    "contractions.add(\"n'\", 'and')\n",
    "contractions.add(\"'n\", 'and')\n",
    "contractions.add(\"pop'n\", \"poping\")\n",
    "contractions.add(\"pop'ettes\", \"popettes\")\n",
    "contractions.add(\"chick'n\", \"chicken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "dp_GeBSvf4qX"
   },
   "outputs": [],
   "source": [
    "def basic_cleaning(input_text):\n",
    "    text = str(input_text)\n",
    "    \n",
    "#   Removing accented characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "#     Converting to lowercase\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    #Doing stop words twice once before expanding once, after expanding\n",
    "    text = remove_stop_words(text)\n",
    "    \n",
    "    #Fixing contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    #Remove punctuation\n",
    "    text = remove_punctuation(text)\n",
    "    text = convert_numbers_to_words(text)\n",
    "    \n",
    "    #Replacing and character\n",
    "    text = text.replace(\"&\", 'and')\n",
    "    text = text.replace(\"\\\\\", '')\n",
    "    text = text.replace(\"%\", \" percent\")\n",
    "    \n",
    "    text = remove_punctuation(text)\n",
    "    text = convert_to_ing_words(text)\n",
    "    \n",
    "    #Second stop words call\n",
    "    text = remove_stop_words(text)    \n",
    "    \n",
    "    text = remove_punctuation(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#product name will be the name from CSV\n",
    "#changed_text will be after basic text processing\n",
    "#Lemmatized_text column is after applying lemmatization on the changed text\n",
    "df['changed_text'] = df['product_name'].apply(basic_cleaning)\n",
    "df['lemmatized_text'] = df['changed_text'].apply(wordnet_lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[['changed_text', 'lemmatized_text']].to_csv('temp-'+ str(datetime.now()) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the tf-idf vector values\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
    "tv_matrix = tv.fit_transform(df['lemmatized_text'].to_numpy())\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "vocab = tv.get_feature_names()\n",
    "vectors = pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)\n",
    "total_df = df.join(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1558)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the cosine similarity matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(tv_matrix)\n",
    "\n",
    "similarity_df = pd.DataFrame(similarity_matrix)\n",
    "total_df = total_df.join(similarity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the top 5 matched products for each product\n",
    "\n",
    "def get_similar_articles(x):\n",
    "    return \", \".join([str(i) for i in df['product_id'].loc[x.argsort()[-6:-1]]])\n",
    "\n",
    "total_df['top_5_similar_products'] = [get_similar_articles(x) for x in similarity_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>top_5_similar_products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>102, 591, 576, 559, 172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>590, 70, 651, 908, 240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>29, 738, 862, 560, 569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>649, 319, 431, 997, 774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>275, 846, 851, 872, 253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>996</td>\n",
       "      <td>725, 627, 359, 585, 222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>997</td>\n",
       "      <td>606, 207, 135, 130, 431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>998</td>\n",
       "      <td>194, 448, 253, 554, 993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>999</td>\n",
       "      <td>244, 303, 176, 520, 553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1000</td>\n",
       "      <td>340, 341, 329, 871, 919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     product_id   top_5_similar_products\n",
       "0             1  102, 591, 576, 559, 172\n",
       "1             2   590, 70, 651, 908, 240\n",
       "2             3   29, 738, 862, 560, 569\n",
       "3             4  649, 319, 431, 997, 774\n",
       "4             5  275, 846, 851, 872, 253\n",
       "..          ...                      ...\n",
       "995         996  725, 627, 359, 585, 222\n",
       "996         997  606, 207, 135, 130, 431\n",
       "997         998  194, 448, 253, 554, 993\n",
       "998         999  244, 303, 176, 520, 553\n",
       "999        1000  340, 341, 329, 871, 919\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df[['product_id', 'top_5_similar_products']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names_of_products(column_name, product_ids_as_string):\n",
    "    all_names = []\n",
    "    \n",
    "    for id_ in product_ids_as_string.split(', '):\n",
    "        similar_name = df.loc[df.product_id == int(id_), column_name].values.item()\n",
    "        all_names.append(str(similar_name))\n",
    "        \n",
    "    return \", \".join(all_names)\n",
    "\n",
    "total_df['similar_product_names'] = total_df['top_5_similar_products'].apply(lambda x: get_names_of_products('product_name', x))\n",
    "total_df['similar_dept_ids'] = total_df['top_5_similar_products'].apply(lambda x: get_names_of_products('department_id', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similar_product_names</th>\n",
       "      <th>product_id</th>\n",
       "      <th>similar_dept_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Danish Butter Cookies, Oreo Cookies and Cream ...</td>\n",
       "      <td>1</td>\n",
       "      <td>19, 1, 19, 19, 19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Almonds Roasted and Salted, Sweet Cooking Rice...</td>\n",
       "      <td>2</td>\n",
       "      <td>19, 6, 16, 13, 13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fresh Cut Golden Sweet No Salt Added Whole Ker...</td>\n",
       "      <td>3</td>\n",
       "      <td>15, 16, 16, 7, 19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Classic coke, Steamfresh Chef's Favorites Ligh...</td>\n",
       "      <td>4</td>\n",
       "      <td>7, 1, 1, 1, 9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Caramel Sauce, Apple Green Cups, Chile Con Que...</td>\n",
       "      <td>5</td>\n",
       "      <td>19, 17, 19, 7, 7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Ceylon Cinnamon, Low Fat Honey Graham Crackers...</td>\n",
       "      <td>996</td>\n",
       "      <td>13, 19, 20, 19, 14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Salted Caramel Ice Cream, Minis Candy Bars, Da...</td>\n",
       "      <td>997</td>\n",
       "      <td>1, 19, 19, 1, 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Lamb Rib Chops, White Chicken Chili, Organic V...</td>\n",
       "      <td>998</td>\n",
       "      <td>12, 15, 7, 20, 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Organic Roasted Garlic Pasta Sauce, Candy, Ori...</td>\n",
       "      <td>999</td>\n",
       "      <td>9, 19, 20, 13, 13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Eco-Pac Kamut Puffs Cereal, Cheese Creations F...</td>\n",
       "      <td>1000</td>\n",
       "      <td>14, 9, 16, 11, 19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 similar_product_names  product_id  \\\n",
       "0    Danish Butter Cookies, Oreo Cookies and Cream ...           1   \n",
       "1    Almonds Roasted and Salted, Sweet Cooking Rice...           2   \n",
       "2    Fresh Cut Golden Sweet No Salt Added Whole Ker...           3   \n",
       "3    Classic coke, Steamfresh Chef's Favorites Ligh...           4   \n",
       "4    Caramel Sauce, Apple Green Cups, Chile Con Que...           5   \n",
       "..                                                 ...         ...   \n",
       "995  Ceylon Cinnamon, Low Fat Honey Graham Crackers...         996   \n",
       "996  Salted Caramel Ice Cream, Minis Candy Bars, Da...         997   \n",
       "997  Lamb Rib Chops, White Chicken Chili, Organic V...         998   \n",
       "998  Organic Roasted Garlic Pasta Sauce, Candy, Ori...         999   \n",
       "999  Eco-Pac Kamut Puffs Cereal, Cheese Creations F...        1000   \n",
       "\n",
       "       similar_dept_ids  \n",
       "0     19, 1, 19, 19, 19  \n",
       "1     19, 6, 16, 13, 13  \n",
       "2     15, 16, 16, 7, 19  \n",
       "3         7, 1, 1, 1, 9  \n",
       "4      19, 17, 19, 7, 7  \n",
       "..                  ...  \n",
       "995  13, 19, 20, 19, 14  \n",
       "996     1, 19, 19, 1, 1  \n",
       "997   12, 15, 7, 20, 15  \n",
       "998   9, 19, 20, 13, 13  \n",
       "999   14, 9, 16, 11, 19  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df[['similar_product_names', 'product_id', 'similar_dept_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df['suggested_products'] = total_df['similar_product_names']\n",
    "total_df['suggested_pro_ids'] = total_df['top_5_similar_products']\n",
    "total_df['suggested_dept_ids'] = total_df['similar_dept_ids']\n",
    "total_df[\n",
    "    ['product_id', \n",
    "     'product_name', \n",
    "     'aisle_id', \n",
    "     'department_id', \n",
    "     'suggested_products', \n",
    "     'suggested_pro_ids', \n",
    "     'suggested_dept_ids']].to_csv('output.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "similar_products.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
