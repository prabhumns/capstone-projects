{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 337,
     "status": "ok",
     "timestamp": 1627173348101,
     "user": {
      "displayName": "MADIPALLI NAGA SAI PRABHU me16b022",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLyKiumkZW5XrfzKvCtZYKsgQizdNYWigyq7b0Qg=s64",
      "userId": "13057357739938460577"
     },
     "user_tz": -330
    },
    "id": "M9VrMr6ZCDDl",
    "outputId": "894dd9d9-81e1-4aa2-aab9-b6cd1523f898"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 589,
     "status": "ok",
     "timestamp": 1627173349088,
     "user": {
      "displayName": "MADIPALLI NAGA SAI PRABHU me16b022",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLyKiumkZW5XrfzKvCtZYKsgQizdNYWigyq7b0Qg=s64",
      "userId": "13057357739938460577"
     },
     "user_tz": -330
    },
    "id": "oGqRtC6dB789"
   },
   "outputs": [],
   "source": [
    "CURR_DIR = '/tf/Capstone Project/project-1' #Local Docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1248,
     "status": "ok",
     "timestamp": 1627173353093,
     "user": {
      "displayName": "MADIPALLI NAGA SAI PRABHU me16b022",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLyKiumkZW5XrfzKvCtZYKsgQizdNYWigyq7b0Qg=s64",
      "userId": "13057357739938460577"
     },
     "user_tz": -330
    },
    "id": "pKiHFFgKB78_",
    "outputId": "128aeb46-33d7-4209-f1f9-49a969691e87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/Capstone Project/Project-1\n",
      "/tf/Capstone Project/project-1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import unicodedata\n",
    "import contractions\n",
    "import os\n",
    "from datetime import datetime\n",
    "from num2words import num2words\n",
    "print(os.getcwd())\n",
    "os.chdir(CURR_DIR)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/Capstone Project/project-1\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pingouin as pg\n",
    "color = sns.color_palette()\n",
    "from pandas.plotting import table\n",
    "import scipy\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import datetime as dt\n",
    "import plotly\n",
    "import cufflinks as cf\n",
    "import plotly.offline\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1627173357917,
     "user": {
      "displayName": "MADIPALLI NAGA SAI PRABHU me16b022",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLyKiumkZW5XrfzKvCtZYKsgQizdNYWigyq7b0Qg=s64",
      "userId": "13057357739938460577"
     },
     "user_tz": -330
    },
    "id": "nNudehDeB79A"
   },
   "outputs": [],
   "source": [
    "def shape_df(df):\n",
    "    print(f\"Number of observations: {df.shape[0]}\")\n",
    "    print(f\"Number of variables: {df.shape[1]}\")\n",
    "    print(f\"Number of duplicates: {df.duplicated().sum()}\")\n",
    "    print(f\"Are there any missing values {df.isnull().values.any()}\")\n",
    "    print(\"-----\")\n",
    "    print(df.dtypes.sort_values(ascending=True))\n",
    "    print(\"------\")\n",
    "    print(\"Datatypes' proportion:\")\n",
    "    print(df.dtypes.value_counts(ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1627173357918,
     "user": {
      "displayName": "MADIPALLI NAGA SAI PRABHU me16b022",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLyKiumkZW5XrfzKvCtZYKsgQizdNYWigyq7b0Qg=s64",
      "userId": "13057357739938460577"
     },
     "user_tz": -330
    },
    "id": "MDEKJ41DB79B"
   },
   "outputs": [],
   "source": [
    "def null_val(df):\n",
    "    detect_null_val = df.isnull().values.any()\n",
    "    if detect_null_val:\n",
    "        null_abs = df.isnull().sum()\n",
    "        null_pc = df.isnull().sum() / df.isnull().shape[0] *100\n",
    "        null_concat = pd.concat([null_abs,null_pc], axis=1).round(2)\n",
    "        null_concat.columns = ['Absolute', 'Percent']\n",
    "        return null_concat.sort_values(by=\"Absolute\", ascending=False)\n",
    "    else:\n",
    "        print(\"There are no missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1627173357919,
     "user": {
      "displayName": "MADIPALLI NAGA SAI PRABHU me16b022",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLyKiumkZW5XrfzKvCtZYKsgQizdNYWigyq7b0Qg=s64",
      "userId": "13057357739938460577"
     },
     "user_tz": -330
    },
    "id": "kX97j6FdB79B"
   },
   "outputs": [],
   "source": [
    "def corrs(x):\n",
    "    mask = np.triu(x.corr(), 1)\n",
    "    plt.figure(figsize=(19, 9))\n",
    "    return sns.heatmap(x.corr(), annot=True, vmax=1, vmin=-1, square=True, cmap='BrBG', mask=mask);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1627173357919,
     "user": {
      "displayName": "MADIPALLI NAGA SAI PRABHU me16b022",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLyKiumkZW5XrfzKvCtZYKsgQizdNYWigyq7b0Qg=s64",
      "userId": "13057357739938460577"
     },
     "user_tz": -330
    },
    "id": "4F4SxGKpB79C"
   },
   "outputs": [],
   "source": [
    "def unique_counts(df, thresh = 15):\n",
    "    for column in df.columns:\n",
    "        if df[column].nunique() < thresh:\n",
    "            print(df.groupby([column], dropna = False).size(), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4825,
     "status": "ok",
     "timestamp": 1627173357916,
     "user": {
      "displayName": "MADIPALLI NAGA SAI PRABHU me16b022",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLyKiumkZW5XrfzKvCtZYKsgQizdNYWigyq7b0Qg=s64",
      "userId": "13057357739938460577"
     },
     "user_tz": -330
    },
    "id": "NMRmcjQ1B79A"
   },
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('./DATA/essay_sample-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>projectid</th>\n",
       "      <th>teacher_acctid</th>\n",
       "      <th>title</th>\n",
       "      <th>short_description</th>\n",
       "      <th>need_statement</th>\n",
       "      <th>essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fffee68353ade53e7692b23098096683</td>\n",
       "      <td>dddd29e34a0e4d889e65ccaa6efc44cf</td>\n",
       "      <td>Tag Readers for Kindergartners</td>\n",
       "      <td>If our classroom were to get these Tag readers...</td>\n",
       "      <td>My students need 8 Tag Readers, Tag Reader boo...</td>\n",
       "      <td>My students enjoy hearing stories and this Tag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fffe598a41270538eab48432513513ab</td>\n",
       "      <td>a2abddbac0cc4a11e8d791469c580d2c</td>\n",
       "      <td>Glazed Clay Pots</td>\n",
       "      <td>My students love to open their imaginations by...</td>\n",
       "      <td>My students need 50lbs of clay and some colorf...</td>\n",
       "      <td>My students love to open their imaginations by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fffe426c60834b0ba2eb05f759d854da</td>\n",
       "      <td>0e853a40819781b2de5d410418674d02</td>\n",
       "      <td>Listening For Fluency</td>\n",
       "      <td>Upgrading our listening center by adding indiv...</td>\n",
       "      <td>My students need 6 listening center headphones...</td>\n",
       "      <td>Fifty per cent of my students have been left b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fffe0bb8af3b9cd93046b49653cc923a</td>\n",
       "      <td>820b80005bdffb74fe85e4deb11c5a07</td>\n",
       "      <td>Building Our Library!</td>\n",
       "      <td>Do you know what it is like to have to come to...</td>\n",
       "      <td>My students need 10 subscriptions to Scholasti...</td>\n",
       "      <td>Do you know what it is like to have to come to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fffdf9d286e715165b60674ac9d05c6c</td>\n",
       "      <td>e4787cea9545eb4393e936504e24d91e</td>\n",
       "      <td>Delete the Digital Drama</td>\n",
       "      <td>Did you know that an estimated 32% of teens re...</td>\n",
       "      <td>My students need an Apple iPad to view the Tee...</td>\n",
       "      <td>Did you know that an estimated 32% of teens re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          projectid                    teacher_acctid  \\\n",
       "0  fffee68353ade53e7692b23098096683  dddd29e34a0e4d889e65ccaa6efc44cf   \n",
       "1  fffe598a41270538eab48432513513ab  a2abddbac0cc4a11e8d791469c580d2c   \n",
       "2  fffe426c60834b0ba2eb05f759d854da  0e853a40819781b2de5d410418674d02   \n",
       "3  fffe0bb8af3b9cd93046b49653cc923a  820b80005bdffb74fe85e4deb11c5a07   \n",
       "4  fffdf9d286e715165b60674ac9d05c6c  e4787cea9545eb4393e936504e24d91e   \n",
       "\n",
       "                            title  \\\n",
       "0  Tag Readers for Kindergartners   \n",
       "1                Glazed Clay Pots   \n",
       "2           Listening For Fluency   \n",
       "3           Building Our Library!   \n",
       "4        Delete the Digital Drama   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  If our classroom were to get these Tag readers...   \n",
       "1  My students love to open their imaginations by...   \n",
       "2  Upgrading our listening center by adding indiv...   \n",
       "3  Do you know what it is like to have to come to...   \n",
       "4  Did you know that an estimated 32% of teens re...   \n",
       "\n",
       "                                      need_statement  \\\n",
       "0  My students need 8 Tag Readers, Tag Reader boo...   \n",
       "1  My students need 50lbs of clay and some colorf...   \n",
       "2  My students need 6 listening center headphones...   \n",
       "3  My students need 10 subscriptions to Scholasti...   \n",
       "4  My students need an Apple iPad to view the Tee...   \n",
       "\n",
       "                                               essay  \n",
       "0  My students enjoy hearing stories and this Tag...  \n",
       "1  My students love to open their imaginations by...  \n",
       "2  Fifty per cent of my students have been left b...  \n",
       "3  Do you know what it is like to have to come to...  \n",
       "4  Did you know that an estimated 32% of teens re...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>projectid</th>\n",
       "      <th>teacher_acctid</th>\n",
       "      <th>title</th>\n",
       "      <th>short_description</th>\n",
       "      <th>need_statement</th>\n",
       "      <th>essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>241538</td>\n",
       "      <td>241538</td>\n",
       "      <td>241529</td>\n",
       "      <td>241486</td>\n",
       "      <td>240960</td>\n",
       "      <td>241538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>241538</td>\n",
       "      <td>130321</td>\n",
       "      <td>217866</td>\n",
       "      <td>237720</td>\n",
       "      <td>234972</td>\n",
       "      <td>241325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>e338b56cacda7d972e271421fdf16771</td>\n",
       "      <td>787720cc575e3204991b8ca18d312d2b</td>\n",
       "      <td>Listening Center</td>\n",
       "      <td>Do you remember what it was like the first tim...</td>\n",
       "      <td>My students need a document camera.</td>\n",
       "      <td>Reading is an essential life skill! I am a rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>213</td>\n",
       "      <td>48</td>\n",
       "      <td>289</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               projectid                    teacher_acctid  \\\n",
       "count                             241538                            241538   \n",
       "unique                            241538                            130321   \n",
       "top     e338b56cacda7d972e271421fdf16771  787720cc575e3204991b8ca18d312d2b   \n",
       "freq                                   1                                95   \n",
       "\n",
       "                   title                                  short_description  \\\n",
       "count             241529                                             241486   \n",
       "unique            217866                                             237720   \n",
       "top     Listening Center  Do you remember what it was like the first tim...   \n",
       "freq                 213                                                 48   \n",
       "\n",
       "                             need_statement  \\\n",
       "count                                240960   \n",
       "unique                               234972   \n",
       "top     My students need a document camera.   \n",
       "freq                                    289   \n",
       "\n",
       "                                                    essay  \n",
       "count                                              241538  \n",
       "unique                                             241325  \n",
       "top     Reading is an essential life skill! I am a rea...  \n",
       "freq                                                    7  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations: 241538\n",
      "Number of variables: 6\n",
      "Number of duplicates: 0\n",
      "Are there any missing values True\n",
      "-----\n",
      "projectid            object\n",
      "teacher_acctid       object\n",
      "title                object\n",
      "short_description    object\n",
      "need_statement       object\n",
      "essay                object\n",
      "dtype: object\n",
      "------\n",
      "Datatypes' proportion:\n",
      "object    6\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "shape_df(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "executionInfo": {
     "elapsed": 3946,
     "status": "ok",
     "timestamp": 1627173367856,
     "user": {
      "displayName": "MADIPALLI NAGA SAI PRABHU me16b022",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiLyKiumkZW5XrfzKvCtZYKsgQizdNYWigyq7b0Qg=s64",
      "userId": "13057357739938460577"
     },
     "user_tz": -330
    },
    "id": "n0-KexWvB79E",
    "outputId": "201463e4-d619-4205-d5f2-180d2a1a5344"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Absolute</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>need_statement</th>\n",
       "      <td>578</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_description</th>\n",
       "      <td>52</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>projectid</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teacher_acctid</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>essay</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Absolute  Percent\n",
       "need_statement          578     0.24\n",
       "short_description        52     0.02\n",
       "title                     9     0.00\n",
       "projectid                 0     0.00\n",
       "teacher_acctid            0     0.00\n",
       "essay                     0     0.00"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_val(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the total size of the dataset we can drop the data points wiht no need_statement or short_descritpion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking only 50000k datapoints to limit the processing time.\n",
    "df = df_raw.copy().sort_values('projectid').head(50000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some text cleaning functions used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing HTML Tags\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    print('Removing HTML Tags, the text can be as big as entire wepage')\n",
    "    return BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "\n",
    "#Removing Accented characters\n",
    "import unicodedata\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    print(\"Removing accented characters, which convert rÃ©sumÃ© to resumve\")\n",
    "    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_text\n",
    "\n",
    "\n",
    "# \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ ðŸ™‚ðŸ™‚ðŸ™‚\" ==> 'Well this was fun See you at  What do you think  '\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    print(\"Removing special characters like smileys from the text\")\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "#Removing stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "new_stop_words = []\n",
    "all_stop_words = stop_words.union(new_stop_words)\n",
    "\n",
    "not_stopwords = {'no', 'not'}\n",
    "final_stop_words = set(\n",
    "    [word for word in all_stop_words if word not in not_stopwords]\n",
    ")\n",
    "def remove_stop_words(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word not in stop_words:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)\n",
    "            \n",
    "#Removing symbols (Without apostrophe)\n",
    "symbols = \"!\\\"#$%&()*+-./:;<=>?,@[\\]^_`{|}~\\n\"\n",
    "def remove_punctuation(text):\n",
    "    for i in symbols:\n",
    "        text = np.char.replace(text, i, ' ')\n",
    "    return str(text)\n",
    "\n",
    "#Convers popin' to poping\n",
    "def convert_to_ing_words(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word[-3:] == \"in'\":\n",
    "            new_text.append(word[:-3] + \"ing\")\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "#Conver Numeric values\n",
    "def convert_numbers_to_words(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if (word.isnumeric()):\n",
    "            new_text.extend(num2words(word).split())\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# My system keeps crashing his crashed yesterday ours crashes daily and presumably we are not lying ==>\n",
    "#PorterStemmer ==>         my system keep crash hi crash yesterday our crash daili and presum we are not lie\n",
    "#LancasterStemmer ==>         my system keep crash his crash yesterday our crash dai and presum we ar not lying\n",
    "ps_ = nltk.porter.PorterStemmer()\n",
    "ls_ = nltk.stem.LancasterStemmer()\n",
    "import nltk\n",
    "def simple_stemming(text, stemmer_type = 'ls'):\n",
    "    if (stemmer_type == 'ps'):\n",
    "        stemmer = ps_\n",
    "    else:\n",
    "        stemmer = ls_\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    return text\n",
    "            \n",
    "    \n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def pos_tag_wordnet(tagged_tokens):\n",
    "    tag_map = {'j': wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV}\n",
    "    new_tagged_tokens = [(word, tag_map.get(tag[0].lower(), wordnet.NOUN))\n",
    "                            for word, tag in tagged_tokens]\n",
    "    return new_tagged_tokens\n",
    "\n",
    "def wordnet_lemmatize_text(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tagged_tokens = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    wordnet_tokens = pos_tag_wordnet(tagged_tokens)\n",
    "    lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['projectid', 'teacher_acctid', 'title', 'short_description',\n",
       "       'need_statement', 'essay'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>essay_word_count</th>\n",
       "      <th>short_description_word_count</th>\n",
       "      <th>need_statement_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.00000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.144860</td>\n",
       "      <td>276.77018</td>\n",
       "      <td>36.990520</td>\n",
       "      <td>19.302440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.241922</td>\n",
       "      <td>88.06406</td>\n",
       "      <td>8.922606</td>\n",
       "      <td>8.903887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>211.00000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>264.00000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>333.00000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>1210.00000</td>\n",
       "      <td>241.000000</td>\n",
       "      <td>397.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       title_word_count  essay_word_count  short_description_word_count  \\\n",
       "count      50000.000000       50000.00000                  50000.000000   \n",
       "mean           5.144860         276.77018                     36.990520   \n",
       "std            2.241922          88.06406                      8.922606   \n",
       "min            1.000000           1.00000                      1.000000   \n",
       "25%            3.000000         211.00000                     33.000000   \n",
       "50%            5.000000         264.00000                     36.000000   \n",
       "75%            7.000000         333.00000                     41.000000   \n",
       "max           21.000000        1210.00000                    241.000000   \n",
       "\n",
       "       need_statement_word_count  \n",
       "count               50000.000000  \n",
       "mean                   19.302440  \n",
       "std                     8.903887  \n",
       "min                     0.000000  \n",
       "25%                    13.000000  \n",
       "50%                    18.000000  \n",
       "75%                    24.000000  \n",
       "max                   397.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def put_word_count_column(df, column_name):\n",
    "    new_column_name = column_name + '_word_count'\n",
    "    df[new_column_name] = df[column_name].apply(lambda s: len(str(s).split()))\n",
    "    return\n",
    "put_word_count_column(df, 'title')\n",
    "put_word_count_column(df, 'essay')\n",
    "put_word_count_column(df, 'short_description')\n",
    "put_word_count_column(df, 'need_statement')\n",
    "print(df.shape)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ima': 'I am going to',\n",
       " 'gonna': 'going to',\n",
       " 'gotta': 'got to',\n",
       " 'wanna': 'want to',\n",
       " 'woulda': 'would have',\n",
       " 'gimme': 'give me',\n",
       " 'asap': 'as soon as possible',\n",
       " 'u': 'you',\n",
       " 'r ': 'are ',\n",
       " 'Im': 'I am',\n",
       " \"I'm\": 'I am',\n",
       " 'Ima': 'I am about to',\n",
       " \"Im'a\": 'I am about to',\n",
       " \"I'ma\": 'I am about to',\n",
       " \"I'm'a\": 'I am about to',\n",
       " 'Imo': 'I am going to',\n",
       " \"Im'o\": 'I am going to',\n",
       " \"I'mo\": 'I am going to',\n",
       " \"I'm'o\": 'I am going to',\n",
       " 'Ive': 'I have',\n",
       " \"I've\": 'I have',\n",
       " 'Illve': 'I will have',\n",
       " \"Ill've\": 'I will have',\n",
       " \"I'llve\": 'I will have',\n",
       " \"I'll've\": 'I will have',\n",
       " 'Idve': 'I would have',\n",
       " \"Id've\": 'I would have',\n",
       " \"I'dve\": 'I would have',\n",
       " \"I'd've\": 'I would have',\n",
       " 'amnt': 'am not',\n",
       " \"amn't\": 'am not',\n",
       " 'aint': 'are not',\n",
       " \"ain't\": 'are not',\n",
       " 'arent': 'are not',\n",
       " \"aren't\": 'are not',\n",
       " 'cause': 'because',\n",
       " \"'cause\": 'because',\n",
       " 'cant': 'cannot',\n",
       " \"can't\": 'cannot',\n",
       " 'cantve': 'cannot have',\n",
       " \"cant've\": 'cannot have',\n",
       " \"can'tve\": 'cannot have',\n",
       " \"can't've\": 'cannot have',\n",
       " 'couldve': 'could have',\n",
       " \"could've\": 'could have',\n",
       " 'couldnt': 'could not',\n",
       " \"couldn't\": 'could not',\n",
       " 'couldntve': 'could not have',\n",
       " \"couldnt've\": 'could not have',\n",
       " \"couldn'tve\": 'could not have',\n",
       " \"couldn't've\": 'could not have',\n",
       " 'darent': 'dare not',\n",
       " \"daren't\": 'dare not',\n",
       " 'daresnt': 'dare not',\n",
       " \"daresn't\": 'dare not',\n",
       " 'dasnt': 'dare not',\n",
       " \"dasn't\": 'dare not',\n",
       " 'didnt': 'did not',\n",
       " \"didn't\": 'did not',\n",
       " 'dont': 'do not',\n",
       " \"don't\": 'do not',\n",
       " 'doesnt': 'does not',\n",
       " \"doesn't\": 'does not',\n",
       " 'eer': 'ever',\n",
       " \"e'er\": 'ever',\n",
       " 'everyones': 'everyone is',\n",
       " \"everyone's\": 'everyone is',\n",
       " 'gont': 'go not',\n",
       " \"gon't\": 'go not',\n",
       " 'hadnt': 'had not',\n",
       " \"hadn't\": 'had not',\n",
       " 'hadntve': 'had not have',\n",
       " \"hadnt've\": 'had not have',\n",
       " \"hadn'tve\": 'had not have',\n",
       " \"hadn't've\": 'had not have',\n",
       " 'hasnt': 'has not',\n",
       " \"hasn't\": 'has not',\n",
       " 'havent': 'have not',\n",
       " \"haven't\": 'have not',\n",
       " 'heve': 'he have',\n",
       " \"he've\": 'he have',\n",
       " 'hellve': 'he will have',\n",
       " \"hell've\": 'he will have',\n",
       " \"he'llve\": 'he will have',\n",
       " \"he'll've\": 'he will have',\n",
       " 'hed': 'he would',\n",
       " \"he'd\": 'he would',\n",
       " 'hedve': 'he would have',\n",
       " \"hed've\": 'he would have',\n",
       " \"he'dve\": 'he would have',\n",
       " \"he'd've\": 'he would have',\n",
       " 'heres': 'here is',\n",
       " \"here's\": 'here is',\n",
       " 'howre': 'how are',\n",
       " \"how're\": 'how are',\n",
       " 'howd': 'how did',\n",
       " \"how'd\": 'how did',\n",
       " 'howdy': 'how do you',\n",
       " \"howd'y\": 'how do you',\n",
       " \"how'dy\": 'how do you',\n",
       " \"how'd'y\": 'how do you',\n",
       " 'hows': 'how is',\n",
       " \"how's\": 'how is',\n",
       " 'howll': 'how will',\n",
       " \"how'll\": 'how will',\n",
       " 'isnt': 'is not',\n",
       " \"isn't\": 'is not',\n",
       " 'tis': 'it is',\n",
       " \"'tis\": 'it is',\n",
       " 'twas': 'it was',\n",
       " \"'twas\": 'it was',\n",
       " 'itll': 'it will',\n",
       " \"it'll\": 'it will',\n",
       " 'itllve': 'it will have',\n",
       " \"itll've\": 'it will have',\n",
       " \"it'llve\": 'it will have',\n",
       " \"it'll've\": 'it will have',\n",
       " 'itd': 'it would',\n",
       " \"it'd\": 'it would',\n",
       " 'itdve': 'it would have',\n",
       " \"itd've\": 'it would have',\n",
       " \"it'dve\": 'it would have',\n",
       " \"it'd've\": 'it would have',\n",
       " 'lets': 'let us',\n",
       " \"let's\": 'let us',\n",
       " 'maam': 'madam',\n",
       " \"ma'am\": 'madam',\n",
       " 'mayve': 'may have',\n",
       " \"may've\": 'may have',\n",
       " 'maynt': 'may not',\n",
       " \"mayn't\": 'may not',\n",
       " 'mightve': 'might have',\n",
       " \"might've\": 'might have',\n",
       " 'mightnt': 'might not',\n",
       " \"mightn't\": 'might not',\n",
       " 'mightntve': 'might not have',\n",
       " \"mightnt've\": 'might not have',\n",
       " \"mightn'tve\": 'might not have',\n",
       " \"mightn't've\": 'might not have',\n",
       " 'mustve': 'must have',\n",
       " \"must've\": 'must have',\n",
       " 'mustnt': 'must not',\n",
       " \"mustn't\": 'must not',\n",
       " 'mustntve': 'must not have',\n",
       " \"mustnt've\": 'must not have',\n",
       " \"mustn'tve\": 'must not have',\n",
       " \"mustn't've\": 'must not have',\n",
       " 'neednt': 'need not',\n",
       " \"needn't\": 'need not',\n",
       " 'needntve': 'need not have',\n",
       " \"neednt've\": 'need not have',\n",
       " \"needn'tve\": 'need not have',\n",
       " \"needn't've\": 'need not have',\n",
       " 'neer': 'never',\n",
       " \"ne'er\": 'never',\n",
       " 'oclock': 'of the clock',\n",
       " \"o'clock\": 'of the clock',\n",
       " 'ol': 'old',\n",
       " \"ol'\": 'old',\n",
       " 'oughtnt': 'ought not',\n",
       " \"oughtn't\": 'ought not',\n",
       " 'oughtntve': 'ought not have',\n",
       " \"oughtnt've\": 'ought not have',\n",
       " \"oughtn'tve\": 'ought not have',\n",
       " \"oughtn't've\": 'ought not have',\n",
       " 'oer': 'over',\n",
       " \"o'er\": 'over',\n",
       " 'shant': 'shall not',\n",
       " \"shan't\": 'shall not',\n",
       " \"sha'nt\": 'shall not',\n",
       " \"sha'n't\": 'shall not',\n",
       " 'shallnt': 'shall not',\n",
       " \"shalln't\": 'shall not',\n",
       " 'shantve': 'shall not have',\n",
       " \"shant've\": 'shall not have',\n",
       " \"shan'tve\": 'shall not have',\n",
       " \"shan't've\": 'shall not have',\n",
       " 'shes': 'she is',\n",
       " \"she's\": 'she is',\n",
       " 'shell': 'she will',\n",
       " \"she'll\": 'she will',\n",
       " 'shed': 'she would',\n",
       " \"she'd\": 'she would',\n",
       " 'shedve': 'she would have',\n",
       " \"shed've\": 'she would have',\n",
       " \"she'dve\": 'she would have',\n",
       " \"she'd've\": 'she would have',\n",
       " 'shouldve': 'should have',\n",
       " \"should've\": 'should have',\n",
       " 'shouldnt': 'should not',\n",
       " \"shouldn't\": 'should not',\n",
       " 'shouldntve': 'should not have',\n",
       " \"shouldnt've\": 'should not have',\n",
       " \"shouldn'tve\": 'should not have',\n",
       " \"shouldn't've\": 'should not have',\n",
       " 'sove': 'so have',\n",
       " \"so've\": 'so have',\n",
       " 'sos': 'so is',\n",
       " \"so's\": 'so is',\n",
       " 'somebodys': 'somebody is',\n",
       " \"somebody's\": 'somebody is',\n",
       " 'someones': 'someone is',\n",
       " \"someone's\": 'someone is',\n",
       " 'somethings': 'something is',\n",
       " \"something's\": 'something is',\n",
       " 'thatre': 'that are',\n",
       " \"that're\": 'that are',\n",
       " 'thats': 'that is',\n",
       " \"that's\": 'that is',\n",
       " 'thatll': 'that will',\n",
       " \"that'll\": 'that will',\n",
       " 'thatd': 'that would',\n",
       " \"that'd\": 'that would',\n",
       " 'thatdve': 'that would have',\n",
       " \"thatd've\": 'that would have',\n",
       " \"that'dve\": 'that would have',\n",
       " \"that'd've\": 'that would have',\n",
       " 'therere': 'there are',\n",
       " \"there're\": 'there are',\n",
       " 'theres': 'there is',\n",
       " \"there's\": 'there is',\n",
       " 'therell': 'there will',\n",
       " \"there'll\": 'there will',\n",
       " 'thered': 'there would',\n",
       " \"there'd\": 'there would',\n",
       " 'theredve': 'there would have',\n",
       " \"thered've\": 'there would have',\n",
       " \"there'dve\": 'there would have',\n",
       " \"there'd've\": 'there would have',\n",
       " 'thesere': 'these are',\n",
       " \"these're\": 'these are',\n",
       " 'theyre': 'they are',\n",
       " \"they're\": 'they are',\n",
       " 'theyve': 'they have',\n",
       " \"they've\": 'they have',\n",
       " 'theyll': 'they will',\n",
       " \"they'll\": 'they will',\n",
       " 'theyllve': 'they will have',\n",
       " \"theyll've\": 'they will have',\n",
       " \"they'llve\": 'they will have',\n",
       " \"they'll've\": 'they will have',\n",
       " 'theyd': 'they would',\n",
       " \"they'd\": 'they would',\n",
       " 'theydve': 'they would have',\n",
       " \"theyd've\": 'they would have',\n",
       " \"they'dve\": 'they would have',\n",
       " \"they'd've\": 'they would have',\n",
       " 'thiss': 'this is',\n",
       " \"this's\": 'this is',\n",
       " 'thosere': 'those are',\n",
       " \"those're\": 'those are',\n",
       " 'tove': 'to have',\n",
       " \"to've\": 'to have',\n",
       " 'wasnt': 'was not',\n",
       " \"wasn't\": 'was not',\n",
       " 'weve': 'we have',\n",
       " \"we've\": 'we have',\n",
       " 'wellve': 'we will have',\n",
       " \"well've\": 'we will have',\n",
       " \"we'llve\": 'we will have',\n",
       " \"we'll've\": 'we will have',\n",
       " 'wedve': 'we would have',\n",
       " \"wed've\": 'we would have',\n",
       " \"we'dve\": 'we would have',\n",
       " \"we'd've\": 'we would have',\n",
       " 'werent': 'were not',\n",
       " \"weren't\": 'were not',\n",
       " 'whatre': 'what are',\n",
       " \"what're\": 'what are',\n",
       " 'whatd': 'what did',\n",
       " \"what'd\": 'what did',\n",
       " 'whatve': 'what have',\n",
       " \"what've\": 'what have',\n",
       " 'whats': 'what is',\n",
       " \"what's\": 'what is',\n",
       " 'whatll': 'what will',\n",
       " \"what'll\": 'what will',\n",
       " 'whatllve': 'what will have',\n",
       " \"whatll've\": 'what will have',\n",
       " \"what'llve\": 'what will have',\n",
       " \"what'll've\": 'what will have',\n",
       " 'whenve': 'when have',\n",
       " \"when've\": 'when have',\n",
       " 'whens': 'when is',\n",
       " \"when's\": 'when is',\n",
       " 'wherere': 'where are',\n",
       " \"where're\": 'where are',\n",
       " 'whered': 'where did',\n",
       " \"where'd\": 'where did',\n",
       " 'whereve': 'where have',\n",
       " \"where've\": 'where have',\n",
       " 'wheres': 'where is',\n",
       " \"where's\": 'where is',\n",
       " 'whichs': 'which is',\n",
       " \"which's\": 'which is',\n",
       " 'whove': 'who have',\n",
       " \"who've\": 'who have',\n",
       " 'whos': 'who is',\n",
       " \"who's\": 'who is',\n",
       " 'wholl': 'who will',\n",
       " \"who'll\": 'who will',\n",
       " 'whollve': 'who will have',\n",
       " \"wholl've\": 'who will have',\n",
       " \"who'llve\": 'who will have',\n",
       " \"who'll've\": 'who will have',\n",
       " 'whod': 'who would',\n",
       " \"who'd\": 'who would',\n",
       " 'whodve': 'who would have',\n",
       " \"whod've\": 'who would have',\n",
       " \"who'dve\": 'who would have',\n",
       " \"who'd've\": 'who would have',\n",
       " 'whyre': 'why are',\n",
       " \"why're\": 'why are',\n",
       " 'whyd': 'why did',\n",
       " \"why'd\": 'why did',\n",
       " 'whyve': 'why have',\n",
       " \"why've\": 'why have',\n",
       " 'whys': 'why is',\n",
       " \"why's\": 'why is',\n",
       " 'willve': 'will have',\n",
       " \"will've\": 'will have',\n",
       " 'wont': 'will not',\n",
       " \"won't\": 'will not',\n",
       " 'wontve': 'will not have',\n",
       " \"wont've\": 'will not have',\n",
       " \"won'tve\": 'will not have',\n",
       " \"won't've\": 'will not have',\n",
       " 'wouldve': 'would have',\n",
       " \"would've\": 'would have',\n",
       " 'wouldnt': 'would not',\n",
       " \"wouldn't\": 'would not',\n",
       " 'wouldntve': 'would not have',\n",
       " \"wouldnt've\": 'would not have',\n",
       " \"wouldn'tve\": 'would not have',\n",
       " \"wouldn't've\": 'would not have',\n",
       " 'yall': 'you all',\n",
       " \"y'all\": 'you all',\n",
       " 'yallre': 'you all are',\n",
       " \"yall're\": 'you all are',\n",
       " \"y'allre\": 'you all are',\n",
       " \"y'all're\": 'you all are',\n",
       " 'yallve': 'you all have',\n",
       " \"yall've\": 'you all have',\n",
       " \"y'allve\": 'you all have',\n",
       " \"y'all've\": 'you all have',\n",
       " 'yalld': 'you all would',\n",
       " \"yall'd\": 'you all would',\n",
       " \"y'alld\": 'you all would',\n",
       " \"y'all'd\": 'you all would',\n",
       " 'yalldve': 'you all would have',\n",
       " \"yalld've\": 'you all would have',\n",
       " \"yall'dve\": 'you all would have',\n",
       " \"yall'd've\": 'you all would have',\n",
       " \"y'alldve\": 'you all would have',\n",
       " \"y'alld've\": 'you all would have',\n",
       " \"y'all'dve\": 'you all would have',\n",
       " \"y'all'd've\": 'you all would have',\n",
       " 'youre': 'you are',\n",
       " \"you're\": 'you are',\n",
       " 'youve': 'you have',\n",
       " \"you've\": 'you have',\n",
       " 'youllve': 'you shall have',\n",
       " \"youll've\": 'you shall have',\n",
       " \"you'llve\": 'you shall have',\n",
       " \"you'll've\": 'you shall have',\n",
       " 'youll': 'you will',\n",
       " \"you'll\": 'you will',\n",
       " 'youd': 'you would',\n",
       " \"you'd\": 'you would',\n",
       " 'youdve': 'you would have',\n",
       " \"youd've\": 'you would have',\n",
       " \"you'dve\": 'you would have',\n",
       " \"you'd've\": 'you would have'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#existing key-value pairs in contractions library\n",
    "contractions.slang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleaning(input_text):\n",
    "    text = str(input_text)\n",
    "    \n",
    "#   Removing accented characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "#     Converting to lowercase\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    #Doing stop words twice once before expanding once, after expanding\n",
    "    text = remove_stop_words(text)\n",
    "    \n",
    "    #Fixing contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    text = text.replace(\"&\", 'and')\n",
    "    text = text.replace(\"%\", \" percent\")\n",
    "    \n",
    "    #Remove punctuation\n",
    "    text = remove_punctuation(text)\n",
    "    text = convert_numbers_to_words(text)\n",
    "    \n",
    "    #Replacing and character\n",
    "\n",
    "    \n",
    "    text = remove_punctuation(text)\n",
    "    text = convert_to_ing_words(text)\n",
    "    \n",
    "    #Second stop words call\n",
    "    text = remove_stop_words(text)    \n",
    "    \n",
    "    text = remove_punctuation(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_text_processing(df, column_name):\n",
    "    print('Processing column: ', column_name)\n",
    "    new_column_name = column_name + \"_changed\"\n",
    "    df[new_column_name] = df[column_name].apply(basic_cleaning)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column:  title\n",
      "Processing column:  short_description\n",
      "Processing column:  need_statement\n",
      "Processing column:  essay\n"
     ]
    }
   ],
   "source": [
    "for col in ['title', 'short_description', 'need_statement', 'essay']: basic_text_processing(df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['projectid', 'teacher_acctid', 'title', 'short_description',\n",
       "       'need_statement', 'essay', 'title_word_count', 'essay_word_count',\n",
       "       'short_description_word_count', 'need_statement_word_count',\n",
       "       'title_changed', 'short_description_changed', 'need_statement_changed',\n",
       "       'essay_changed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('After basic text processing.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>need_statement_changed</th>\n",
       "      <th>short_description_changed</th>\n",
       "      <th>title_changed</th>\n",
       "      <th>essay_changed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>241537</th>\n",
       "      <td>students need word building centers twenty sea...</td>\n",
       "      <td>students need incentives make better faster re...</td>\n",
       "      <td>speedy shark reading club</td>\n",
       "      <td>students need incentives make better faster re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241536</th>\n",
       "      <td>students need high quality books worth chasing...</td>\n",
       "      <td>students need high quality books worth chasing...</td>\n",
       "      <td>go read hrs</td>\n",
       "      <td>students need challenge read high quality exce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241535</th>\n",
       "      <td>students need ipad mini support instruction st...</td>\n",
       "      <td>kindergarten exciting time learning especially...</td>\n",
       "      <td>technology upgrade must kindergartners ready</td>\n",
       "      <td>kindergarten exciting time learning especially...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241534</th>\n",
       "      <td>students need books native americans including...</td>\n",
       "      <td>remember learning martin luther king jr librar...</td>\n",
       "      <td>innovative lessons civil rights diversity</td>\n",
       "      <td>remember learning martin luther king jr librar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241533</th>\n",
       "      <td>students need twenty two weather related books...</td>\n",
       "      <td>study weather fifth grade curriculum would lik...</td>\n",
       "      <td>meteorology matters</td>\n",
       "      <td>fabulous children school love research since t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   need_statement_changed  \\\n",
       "241537  students need word building centers twenty sea...   \n",
       "241536  students need high quality books worth chasing...   \n",
       "241535  students need ipad mini support instruction st...   \n",
       "241534  students need books native americans including...   \n",
       "241533  students need twenty two weather related books...   \n",
       "\n",
       "                                short_description_changed  \\\n",
       "241537  students need incentives make better faster re...   \n",
       "241536  students need high quality books worth chasing...   \n",
       "241535  kindergarten exciting time learning especially...   \n",
       "241534  remember learning martin luther king jr librar...   \n",
       "241533  study weather fifth grade curriculum would lik...   \n",
       "\n",
       "                                       title_changed  \\\n",
       "241537                     speedy shark reading club   \n",
       "241536                                   go read hrs   \n",
       "241535  technology upgrade must kindergartners ready   \n",
       "241534     innovative lessons civil rights diversity   \n",
       "241533                           meteorology matters   \n",
       "\n",
       "                                            essay_changed  \n",
       "241537  students need incentives make better faster re...  \n",
       "241536  students need challenge read high quality exce...  \n",
       "241535  kindergarten exciting time learning especially...  \n",
       "241534  remember learning martin luther king jr librar...  \n",
       "241533  fabulous children school love research since t...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['need_statement_changed', 'short_description_changed', 'title_changed', 'essay_changed']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemm_(df, column_name):\n",
    "    print('Processing column: ', column_name)\n",
    "    new_column_name = column_name + \"_stemmed\"\n",
    "    df[new_column_name] = df[column_name].apply(simple_stemming)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column:  need_statement_changed\n",
      "Processing column:  short_description_changed\n",
      "Processing column:  title_changed\n",
      "Processing column:  essay_changed\n"
     ]
    }
   ],
   "source": [
    "for col in ['need_statement_changed', 'short_description_changed', 'title_changed', 'essay_changed']: stemm_(df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>need_statement_changed_stemmed</th>\n",
       "      <th>short_description_changed_stemmed</th>\n",
       "      <th>title_changed_stemmed</th>\n",
       "      <th>essay_changed_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>241537</th>\n",
       "      <td>stud nee word build cent twenty seat stor sack</td>\n",
       "      <td>stud nee int mak bet fast read read system cur...</td>\n",
       "      <td>speedy shark read club</td>\n",
       "      <td>stud nee int mak bet fast read read system cur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241536</th>\n",
       "      <td>stud nee high qual book wor chas verm goos gir...</td>\n",
       "      <td>stud nee high qual book wor chas verm goos gir...</td>\n",
       "      <td>go read hrs</td>\n",
       "      <td>stud nee challeng read high qual excel book n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241535</th>\n",
       "      <td>stud nee ipad min support instruct stay track ...</td>\n",
       "      <td>kindergart excit tim learn espec access child ...</td>\n",
       "      <td>technolog upgrad must kindergartn ready</td>\n",
       "      <td>kindergart excit tim learn espec access child ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241534</th>\n",
       "      <td>stud nee book nat am includ thirty cop soft ra...</td>\n",
       "      <td>rememb learn martin luth king jr libr classroo...</td>\n",
       "      <td>innov lesson civil right divers</td>\n",
       "      <td>rememb learn martin luth king jr libr classroo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241533</th>\n",
       "      <td>stud nee twenty two weath rel book act book in...</td>\n",
       "      <td>study weath fif grad curricul would lik stud r...</td>\n",
       "      <td>meteorolog mat</td>\n",
       "      <td>fab childr school lov research sint titl on sc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           need_statement_changed_stemmed  \\\n",
       "241537     stud nee word build cent twenty seat stor sack   \n",
       "241536  stud nee high qual book wor chas verm goos gir...   \n",
       "241535  stud nee ipad min support instruct stay track ...   \n",
       "241534  stud nee book nat am includ thirty cop soft ra...   \n",
       "241533  stud nee twenty two weath rel book act book in...   \n",
       "\n",
       "                        short_description_changed_stemmed  \\\n",
       "241537  stud nee int mak bet fast read read system cur...   \n",
       "241536  stud nee high qual book wor chas verm goos gir...   \n",
       "241535  kindergart excit tim learn espec access child ...   \n",
       "241534  rememb learn martin luth king jr libr classroo...   \n",
       "241533  study weath fif grad curricul would lik stud r...   \n",
       "\n",
       "                          title_changed_stemmed  \\\n",
       "241537                   speedy shark read club   \n",
       "241536                              go read hrs   \n",
       "241535  technolog upgrad must kindergartn ready   \n",
       "241534          innov lesson civil right divers   \n",
       "241533                           meteorolog mat   \n",
       "\n",
       "                                    essay_changed_stemmed  \n",
       "241537  stud nee int mak bet fast read read system cur...  \n",
       "241536  stud nee challeng read high qual excel book n ...  \n",
       "241535  kindergart excit tim learn espec access child ...  \n",
       "241534  rememb learn martin luth king jr libr classroo...  \n",
       "241533  fab childr school lov research sint titl on sc...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['need_statement_changed_stemmed','short_description_changed_stemmed', 'title_changed_stemmed', 'essay_changed_stemmed']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('After Stemming.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizers = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vectorize_column(df, column, tv):\n",
    "    print('Given shape: ',df.shape)\n",
    "    tv_matrix = tv.fit_transform(df[column].to_numpy())\n",
    "    tv_matrix = tv_matrix.toarray()\n",
    "    vocab = [column + '_' + i for i in tv.get_feature_names()]\n",
    "    vectors = pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)\n",
    "    print('Shape of vectors generated: ', vectors.shape)\n",
    "    return df.join(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given shape:  (50000, 18)\n",
      "Shape of vectors generated:  (50000, 200)\n",
      "Final shape:  (50000, 218)\n"
     ]
    }
   ],
   "source": [
    "#For Title Column\n",
    "vectorizers['title'] = TfidfVectorizer(min_df = 0, max_df = 1., use_idf = True, max_features = 200)\n",
    "df = add_vectorize_column(df, 'title_changed_stemmed', vectorizers['title'])\n",
    "print('Final shape: ', df.shape)\n",
    "df.to_csv('After title vectorization.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given shape:  (50000, 218)\n",
      "Shape of vectors generated:  (50000, 500)\n",
      "Final shape:  (50000, 718)\n"
     ]
    }
   ],
   "source": [
    "#For need statement\n",
    "vectorizers['need_statement'] = TfidfVectorizer(min_df = 0, max_df = 1., use_idf = True, max_features = 500)\n",
    "df = add_vectorize_column(df, 'need_statement_changed_stemmed', vectorizers['need_statement'])\n",
    "print('Final shape: ', df.shape)\n",
    "df.to_csv('After need statement vectorization.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given shape:  (50000, 718)\n",
      "Shape of vectors generated:  (50000, 500)\n",
      "Final shape:  (50000, 1218)\n"
     ]
    }
   ],
   "source": [
    "#For need statement\n",
    "vectorizers['short_description'] = TfidfVectorizer(min_df = 0, max_df = 1., use_idf = True, max_features = 500)\n",
    "df = add_vectorize_column(df, 'short_description_changed_stemmed', vectorizers['short_description'])\n",
    "print('Final shape: ', df.shape)\n",
    "df.to_csv('After short description vectorisation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given shape:  (50000, 1218)\n",
      "Shape of vectors generated:  (50000, 1000)\n",
      "Final shape:  (50000, 2218)\n"
     ]
    }
   ],
   "source": [
    "#For need statement\n",
    "vectorizers['essays'] = TfidfVectorizer(min_df = 0, max_df = 1., use_idf = True, max_features = 1000)\n",
    "df = add_vectorize_column(df, 'essay_changed_stemmed', vectorizers['essays'])\n",
    "print('Final shape: ', df.shape)\n",
    "df.to_csv('After essays vectorisation.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DonorsChoose.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
